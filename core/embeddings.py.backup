"""
Generation d'embeddings pour les documents
"""
import json
import hashlib
from pathlib import Path
from typing import List, Optional
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential
from config.settings import settings
from utils.logger import log


class EmbeddingGenerator:
    """Generateur d'embeddings avec cache"""
    
    def __init__(self):
        """Initialise le client OpenAI"""
        if not settings.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY manquante dans .env")
        
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.EMBEDDING_MODEL
        self.cache_dir = settings.CACHE_DIR / "embeddings_cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        log.info(f"Embedding generator initialise avec modele: {self.model}")
    
    def _get_cache_key(self, text: str) -> str:
        """Genere une cle de cache unique"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[List[float]]:
        """Recupere un embedding du cache"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r') as f:
                    data = json.load(f)
                    log.debug(f"Cache hit embedding: {cache_key}")
                    return data['embedding']
            except Exception as e:
                log.warning(f"Erreur lecture cache embedding: {e}")
        return None
    
    def _save_to_cache(self, cache_key: str, embedding: List[float]):
        """Sauvegarde un embedding dans le cache"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w') as f:
                json.dump({'embedding': embedding}, f)
            log.debug(f"Cache saved embedding: {cache_key}")
        except Exception as e:
            log.warning(f"Erreur ecriture cache embedding: {e}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    def generate(self, text: str, use_cache: bool = True) -> List[float]:
        """
        Genere un embedding pour un texte
        
        Args:
            text: Texte a embedder
            use_cache: Utiliser le cache ou non
            
        Returns:
            Vecteur d'embedding
        """
        # Verifier le cache
        if use_cache:
            cache_key = self._get_cache_key(text)
            cached_embedding = self._get_from_cache(cache_key)
            if cached_embedding:
                return cached_embedding
        
        # Appel API
        try:
            log.debug(f"Generation embedding pour texte de {len(text)} caracteres")
            
            response = self.client.embeddings.create(
                model=self.model,
                input=text
            )
            
            embedding = response.data[0].embedding
            
            # Sauvegarder dans le cache
            if use_cache:
                self._save_to_cache(cache_key, embedding)
            
            log.debug(f"Embedding genere: dimension {len(embedding)}")
            
            return embedding
            
        except Exception as e:
            log.error(f"Erreur generation embedding: {e}")
            raise
    
    def generate_batch(self, texts: List[str], use_cache: bool = True) -> List[List[float]]:
        """
        Genere des embeddings pour plusieurs textes
        
        Args:
            texts: Liste de textes
            use_cache: Utiliser le cache ou non
            
        Returns:
            Liste de vecteurs d'embedding
        """
        embeddings = []
        
        for text in texts:
            embedding = self.generate(text, use_cache=use_cache)
            embeddings.append(embedding)
        
        log.info(f"Batch embeddings generes: {len(embeddings)} vecteurs")
        
        return embeddings


# Instance globale
embedding_generator = EmbeddingGenerator()
